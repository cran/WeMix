<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Introduction</h1>

<p>The \texttt{WeMix} package fits a \texttt{\textbf{We}}ighted \texttt{\textbf{Mix}}ed model, also known as a multilevel, mixed, or hierarchical linear model (HLM). The weights could be inverse selection probabilities, such as those developed for an education survey where schools are sampled probabilistically, and then students inside of those schools are sampled probabilistically. Although mixed-effects models are already available in R, \texttt{WeMix} is unique in implementing methods for mixed models using weights at multiple levels.</p>

<p>The package \texttt{lme4} fits these models when there are no weights or weights only for first-level units (Bates, Maechler, Bolker, &amp; Walker, 2015) and is recommended for those situations&mdash;by default, \texttt{WeMix} uses the \texttt{lme4} results as a starting point. \texttt{WeMix} adds the ability to fit models with weights at every level of the model, similar to GLLAMM (Rabe-Hesketh, Skrondal, &amp; Pickles, 2002; Rabe-Hesketh, Skrondal, &amp; Pickles, 2005; Rabe-Hesketh &amp; Skrondal, 2006) and maximizes the same likelihood function as GLLAMM. Because the model applies weights at every level, the  units must be nested in &ldquo;levels&rdquo;, so \texttt{WeMix} only fits models where the units have a nested structure.</p>

<h2>Installing and Loading \texttt{WeMix}</h2>

<p>Inside R, run the following command to install  \texttt{WeMix}:</p>

<pre><code class="r">install.packages(&quot;WeMix&quot;)
</code></pre>

<p>Once the package is successfully installed, \texttt{WeMix} can be loaded with the following command:</p>

<pre><code class="r">library(WeMix)
</code></pre>

<h2>Specifying a Mixed-Effects Model</h2>

<p>To illustrate the functionality of WeMix, we will use an example based on publicly available data from the Programme for International Student Assessment (PISA) 2012 data from the United States (OECD, 2013). PISA is a repeated multinational assessment of skills and attitudes of 15-year-olds, with students (the unit of observation) probabilistically sampled within schools (the second-level unit) that are also probabilistically sampled within the country. In the United States,  there were 3,136 student observations in 157 schools. We provide examples of a model with a random intercept and a model with both a random slope and intercept.</p>

<p>The first model can be specified as the math assessment predicted by a few variables chosen from the PISA survey:</p>

<ul>
<li>Dependent variable: \texttt{pv1math}, a math assessment score</li>
<li>Independent variables:

<ul>
<li>\texttt{escs}: a continuous Socio-Economic Status index</li>
<li>\texttt{sc14q02} school questionnaire item:  &ldquo;Is your school&#39;s capacity to provide instruction hindered by any of the following&hellip; A lack of qualified mathematics teachers.&rdquo; (levels: &ldquo;A lot&rdquo;; &ldquo;To some extent&rdquo;; &ldquo;Very little&rdquo;; &ldquo;Not at all&rdquo;, the reference gorup)</li>
<li>\texttt{st04q02} student gender (levels: &ldquo;Male&rdquo; and &ldquo;Female&rdquo;, the reference group)</li>
<li>\texttt{st29q03} student questionnaire item: &ldquo;I look forward to math lessons&rdquo; (levels: &ldquo;Strongly agree&rdquo;; &ldquo;Agree&rdquo;; &ldquo;Disagree&rdquo;; &ldquo;Strongly disagree&rdquo;, the reference group)</li>
</ul></li>
<li>School level random effect: school intercept</li>
<li>Student level weights: \texttt{pwt1}</li>
<li>School level weights: \texttt{pwt2}</li>
</ul>

<p>In the second model, a second random effect is added at the school level for the \texttt{escs} variable, but there is no covariance between the two random effects.</p>

<p>Where estimation is done by quadrature, 27 quadrature points are used for the one random effect model and 13 for the two random effects model. </p>

<p>An appendix describes the transformation used to prepare the data for analysis. </p>

<p>The WeMix call for this model, using a <code>data.frame</code> containing the PISA 2012 data for the United States and named &ldquo;data&rdquo;, would be: </p>

<pre><code class="r"># model with one random effect
mix(pv1math ~ st29q03 + sc14q02 +st04q01+escs+ (1|schoolid), data=data, 
    weights=c(&quot;pwt1&quot;, &quot;pwt2&quot;), nQuad=27, verbose=FALSE, fast=TRUE,run=TRUE)

# model with two random effects
mix(pv1math ~ st29q03 + sc14q02 +st04q01+escs+ (1|schoolid)+ (0+escs|schoolid), data=data, 
    weights=c(&quot;pwt1&quot;, &quot;pwt2&quot;), nQuad=13, verbose=FALSE, fast=TRUE,run=TRUE)
</code></pre>

<p>Note that the syntax for the model formula is the same syntax one would use to specify a model using \texttt{lme4}. Thus, in the random slope and intercept model, the slope and intercept are in separate terms in order to constrain their covariance to be zero. </p>

<h2>Comparison to Alternate Software</h2>

<p>For readers familiar with specification of other software, this section shows results in comparison with those from  Stata, SAS, and HLM. When possible, the code specifies a random slope model and then a random slope and intercept model with the covariance of slope and intercept fixed to be zero. The models are fitted by maximum likelihood estimation and example code in Stata GLLAMM, Stata MIXED, SAS GLIMMIX, and HLM are shown in the appendix.</p>

<p>Table 1 shows the results of the model with a single random effect where \texttt{WeMix}, GLLAMM, Stata MIXED, and SAS show agreement on the likelihood, variance estimates of random effects, and fixed effects.  HLM normalizes the weights (for both students and schools) and so produces somewhat different results. Although the estimates are very similar (as reported here to the fith digit), there are some differences in the standard errors of the random effects. Most notably, Stata MIXED calculates a lower standard error for the random intercept than other methods, and  HLM does not calculate standard errors for random effects. All the programs differ somewhat in the standard errors estimated for the fixed effects, and \texttt{WeMix} most closely matches the results from GLLAMM.</p>

<p>Table 2 shows the results of the model with two random effects. Here the results are similar.\texttt{WeMix}, GLLAMM, Stata MIXED, and SAS show agreement on the likelihood, variance term estimates of random effects, and fixed effects. HLMs fit a different model and so get a different result. Similar to the one random effect model, Stata MIXED reports a lower standard error for the random intercept. It is, however, noteworthy that Stata MIXED reports a higher standard error for the random slope than the other methods. In terms of the standard errors of the fixed effects, there are differences between the estimates of all the programs, and \texttt{WeMix} again most closely matches GLLAMM. </p>

<p>\definecolor{header}{rgb}{0.52,.44,1}
\begin{table}[htb]
\centering</p>

<p>\begin{tabular}{|l|r|r|r|r|r|} </p>

<p>\hline 
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Table 1: Results for Random Intercept Model } }\
\hline 
\rowcolor{header}
&amp; WeMix &amp; Stata: GLLAMM  &amp; Stata: MIXED  &amp; SAS &amp; HLM\(^1\) \
\hline
Run Time &amp; 00:31 &amp; 02:00 &amp; 00:30 &amp; 00:03 &amp; 00:10 \
\hline 
Log-Likelihood &amp; -2578035.4  &amp; -2578035.4 &amp; -2578035.4 &amp; -2578035.5 &amp; -17965.4 \ 
\hline 
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Random Effects} }\
\hline 
Var Random Intercept &amp; 1106.6 &amp; 1106.6 &amp; 1106.6  &amp; 1106.6 &amp;  1020.0\ 
Var Residual  &amp; 5109.5 &amp; 5109.5 &amp; 5109.5 &amp; 5109.5 &amp; 5061.1 \
\hline 
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Standard Error (SE) of Random Effects}} \
SE Random Intercept &amp; 289.88 &amp; 289.88 &amp; 280.80 &amp; 288.95 &amp; Not Calculated\ 
SE Residual &amp;  202.53 &amp; 202.53 &amp; 201.88 &amp; 201.88 &amp;  Not Calculated \
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Fixed Effects}} \
\hline 
(Intercept)    &amp; 494.73 &amp; 494.73 &amp; 494.73 &amp; 494.73 &amp; 494.47 \
st29q03: Agree        &amp;  -24.115   &amp; -24.115 &amp; -24.115 &amp; -24.115 &amp; -24.119  \
st29q03: Strongly disagree  &amp;    -54.678  &amp; -54.678 &amp; -54.678 &amp; -54.678 &amp; -54.788 \
st29q03: Disagree  &amp; -26.040  &amp; -26.040  &amp; -26.040 &amp; -26.040 &amp; -26.109\
sc14q02: A lot      &amp;    -24.507   &amp;  -24.507 &amp;  -24.507 &amp; -24.507 &amp;  -24.504\
sc14q02: To some extent &amp; -10.787     &amp; -10.787 &amp; -10.787 &amp; 10.787 &amp; -10.362\
sc14q02: Very little  &amp;  -18.035  &amp; -18.035 &amp; -18.035 &amp;  -18.035 &amp; -19.505 \
st04q01: Male       &amp;     12.127     &amp;  12.127  &amp;  12.127  &amp; 12.127 &amp; 12.164\
escs             &amp;      28.729    &amp;   28.729  &amp;   28.729 &amp; 28.729 &amp; 28.332 \</p>

<p>\hline
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Standard Error of Fixed Effects}} \
\hline 
SE (Intercept)    &amp; 7.1817 &amp; 7.1818 &amp; 7.0885  &amp; 7.1586 &amp; 8.6259 \
SE st29q03: Agree       &amp; 7.2196 &amp; 7.2196  &amp; 7.2182  &amp; 7.1966 &amp; 7.1401 \
SE st29q03: Strongly disagree   &amp; 12.234 &amp; 12.2335  &amp; 12.261 &amp;12.1945 &amp;  12.108\
SE st29q03: Disagree  &amp; 6.3425  &amp; 6.3425 &amp; 6.3558   &amp; 6.3223 &amp; 6.2607 \
SE sc14q02: A lot    &amp; 8.1732 &amp; 8.1732 &amp; 8.1871 &amp; 8.1471 &amp; 7.2026 \
SE sc14q02: To some extent  &amp; 13.646 &amp; 13.646 &amp; 13.528 &amp; 13.603 &amp; 13.587 \
SE sc14q02: Very little   &amp; 14.020 &amp; 14.020 &amp; 14.116 &amp; 13.975 &amp; 15.285 \
SE st04q01: Male        &amp; 3.7203 &amp; 3.7203 &amp; 3.7305 &amp; 3.7085 &amp; 3.7008 \
SE escs              &amp; 2.6071 &amp; 2.6071 &amp; 2.5621 &amp; 2.5988 &amp;  2.5249 \
\hline
\multicolumn{6}{|l|}{\(^1\) HLM requires that the weights are normalized before they are fit so results do not match exactly.} \
\multicolumn{6}{|l|}{Also, HLM cannot set slope and intercept covariance to 0 (Raudenbush, Bryk, Cheong, Congdon, &amp; Toit, 2016).}\
\hline
\end{tabular}
\end{table}
\clearpage</p>

<p>\begin{table}
\centering
\begin{tabular}{|l|r|r|r|r|r|} 
\hline
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Table 2: Results for Random Intercept and Slope Model} }\
\hline 
\rowcolor{header}
&amp; WeMix &amp; Stata: GLLAMM  &amp; Stata: MIXED  &amp; SAS &amp; HLM\(^1\)  \
\hline
Run Time &amp; 3:42 &amp; 20:00 &amp; 00:30 &amp; 00:10 &amp; 00:10 \
\hline 
Log-Likelihood &amp; -25777729.6 &amp; -25777729.6 &amp; -25777729.6 &amp; -2577729.5 &amp;  -17964.0 \ 
\hline 
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Random Effects} }\</p>

<p>\hline 
Var of Intercept &amp; 1075.1 &amp; 1075.1 &amp; 1075.1  &amp; 1075.11 &amp;  984.87 \ 
Var of Slope of escs &amp; 94.036 &amp;  94.034 &amp;  94.035 &amp; 94.036 &amp;  56.277\ 
Var of Residual  &amp; 5048.6 &amp; 5048.6 &amp; 5048.6 &amp; 5048.6 &amp; 5010.3 \
\hline 
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Standard Error (SE) of Random Effects}} \
SE Random Intercept &amp; 271.05  &amp; 271.06 &amp; 265.38 &amp; 270.19 &amp; Not Calculated  \ 
SE Slope of escs &amp; 80.129  &amp; 80.130 &amp; 82.31 &amp; 79.873 &amp;  Not Calculated  \
SE Residual  &amp; 185.63 &amp; 185.63 &amp; 184.57 &amp; 185.04 &amp;  Not Calculated \
\hline
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Fixed Effects}} \
\hline 
(Intercept)    &amp; 494.22 &amp; 494.22 &amp; 494.22 &amp; 494.22 &amp; 494.03 \
st29q03: Agree        &amp;  -23.993   &amp; -23.993 &amp; -23.993 &amp; -23.993 &amp; -24.012  \
st29q03: Strongly disagree  &amp;    -54.330  &amp;  -54.330 &amp;  -54.330 &amp; -54.330 &amp; -54.471 \
st29q03: Disagree  &amp; -25.813  &amp; -25.813  &amp; -25.813 &amp; -25.813 &amp; -27.782 \
sc14q02: A lot      &amp;    -28.232   &amp;   -28.232 &amp;   -28.232 &amp; -28.232 &amp;  -27.782\
sc14q02: To some extent &amp; -11.680     &amp; -11.680  &amp; -11.680 &amp; -11.680 &amp; -11.048\
sc14q02: Very little  &amp;  -18.076  &amp;  -18.076 &amp;  -18.075 &amp;  -18.076 &amp; -19.513 \
st04q01: Male       &amp;     12.230     &amp;   12.230  &amp;   12.230   &amp; 12.230 &amp; 12.252\
escs             &amp;      29.096    &amp;   29.096  &amp;   29.096 &amp; 29.096 &amp; 28.625 \
\hline
\rowcolor{header}
\multicolumn{6}{|c|}{\bf{Standard Error of Fixed Effects}} \
\hline 
SE (Intercept)    &amp; 7.1473 &amp; 7.1473 &amp; 7.0273 &amp; 7.1243 &amp;  7.1488 \
SE st29q03: Agree       &amp; 7.2000 &amp; 7.2000  &amp; 7.1979 &amp;7.1770 &amp; 7.1235 \
SE st29q03: Strongly disagree   &amp; 12.324 &amp; 12.434 &amp; 12.340 &amp; 12.395 &amp; 12.179 \
SE st29q03: Disagree  &amp; 6.3242 &amp; 6.3242 &amp; 6.2991   &amp; 6.3040 &amp; 7.3716 \
SE sc14q02: A lot    &amp; 7.8182  &amp; 7.8181 &amp; 7.2185 &amp; 7.7931 &amp; 7.3716 \
SE sc14q02: To some extent  &amp; 13.622 &amp; 13.621 &amp; 13.407 &amp; 13.5782 &amp; 13.497 \
SE sc14q02: Very little   &amp; 13.728 &amp; 13.728 &amp; 13.835 &amp; 13.6838 &amp;  15.008 \
SE st04q01: Male        &amp; 3.7252 &amp; 3.7252 &amp; 3.7381 &amp; 3.7134 &amp; 3.7084  \
SE escs              &amp; 2.6056 &amp; 2.6056 &amp; 2.6763 &amp; 2.5973 &amp; 2.6105 \
\hline
\multicolumn{6}{|l|}{\(^1\) HLM requires that the weights are normalized before they are fit, so results do not match exactly.}\
\multicolumn{6}{|l|}{Also, HLM cannot set slope and intercept covariance to 0 (Raudenbush, Bryk, Cheong, Congdon, &amp; Toit, 2016).} \
\hline
\end{tabular}
\end{table}
\clearpage</p>

<h2>Mathematical Specification</h2>

<p>This section describes the mathematical methodology behind the estimation of Weighted Mixed models in \texttt{WeMix}.</p>

<p>The simplest version of this model has two levels and is of the form
\begin{equation} \label{eq:basic}
\bm{y} = \bm{X\beta} + \bm{Z u}  + \bm{\epsilon} \ ,
\end{equation}
where \(\bm{y}\) is the vector of outcomes, \(\bm{X}\) is s a matrix of covariates associated with regressors that are assumed to be fixed, \(\bm{\beta}\) is the vector of fixed-effect regression coefficients, \(\bm{Z}\) is a matrix of covariates associated with regressors that are assumed to be random, and \(\bm{u}\) is the vector of random effects. The meaning of \(\bm{u}\) being random is simply that a level is shared within a group and across groups \(\bm{u} \sim MVN(\vec{\bm{0}},\bm{\Sigma})\), where \(MVN(\cdot,\cdot)\) is the multivariate-normal distribution, \(\vec{\bm{0}}\) is the mean vector of all zeros, and \(\bm{\Sigma}\) is the covariance matrix of the MVN.</p>

<h2>Hierarchical Linear Models Notation</h2>

<p>The models \texttt{WeMix} fits can also be called hierarchical linear model (HLMS; Radenbush &amp; Bryk, 2002) where the first level is of the form:
\begin{equation} \label{eq:HLMl1}
y<em>{ij} = \beta</em>{0j} + X\beta<em>{1j}  + \epsilon</em>{ij} \ ,
\end{equation}
So, the second level could then be, for a random slope and intercept model:
\begin{align} \label{eq:HLMl2}
\beta<em>{0j} &amp;= \gamma</em>{00} + \delta<em>{0j} \
\beta</em>{1j} &amp;= \gamma<em>{01} + \delta</em>{1j} \ ,
\end{align}
where \(\delta_{0j}\) and \(\delta_{1j}\) are the error terms for the intercept and slope, respectively, and have variances of \(\tau_{00}\) and \(\tau_{11}\), respectively, and \(\delta_{0j}\) and \(\delta_{1j}\) have covariance \(\tau_{01}\).</p>

<p>This is just one example; many other models can also be fit in \texttt{WeMix}. WeMix can fit models that are stated as an HLM or as a mixed model. For notational convenience for the rest of this document, we will use the non-HLM mixed model notation.</p>

<h2>Multiple Levels</h2>

<p>When there are more than two levels, eq. \ref{eq:basic} can be rewritten as
\begin{equation} \label{eq:full}
\bm{y} = \bm{X\beta} + \sum<sup>L_{l</sup> = 2}\bm{Z}<sup>{(l)}\bm{u}<sup>{(l)}</sup></sup> + \bm{\epsilon} 
\end{equation}
where a superscript \((l)\) is added to \(\bm{Z}\) and \(\bm{u}\) to indicate that they are at the $l$th level. Note: In the summation above, \(l\) starts at \(l=2\) because there cannot be random effects at the lowest level of observation (\(l=1\)).</p>

<p>\begin{table}[htb]
\caption{Notation}
\centering
\begin{tabular}{rp{4in}} 
\hline
\(\bm{\theta}\) &amp; vector of all of the fit model parameters fit, including, \(\bm{\beta}\) and the nonduplicated elements of the covariate matrices \(\bm{\Sigma}^{(l)}\). Because they are integrated out, \(\bm{u}\) is included in \(\bm{\theta}\). \
\(\bm{\Sigma}^{(l)}\) &amp; covariance matrix for level \(l\)\ 
\(\bm{y}\) &amp; response vector \ 
\(\bm{X}\) &amp; covariate matrix for fixed effects  \ 
\(\bm{\beta}\) &amp; coefficients of the fixed effects \
\(\bm{Z}^{(l)}\) &amp; covariate matrix for random effects at level \(l\)\
\(\bm{Z}\) &amp; covariate matrix for all random effects\
\(\bm{u}^{(l)}\) &amp; vector of the random effects at level \(l\) \
\(\bm{U}^{(l)}\) &amp; vector of the random effects at all levels \(l\) and higher, \((u^{l}, &hellip;, u^{L})&lsquo;\) \
\(\bm{w}^{(l)}\) &amp; vector of the weights at level \(l\)\
\(\bm{W}\) &amp; matrix of the weights at all levels\
\(k_l\) &amp; number of random effects at level \(l\) \
\(L\) &amp; number of levels in the model \
\(i\) &amp; subscript denoting the individual \
\(j\) &amp; subscript denoting group\
\(j&rsquo;\) &amp; subscript denoting a group within \(j\) \
\(\mathcal{L}^{(l)}\) &amp; likelihood function at level \(l\) \
\(\ell^{(l)}\) &amp; log-likelihood function at level \(l\) \
\(\bm{\epsilon}\) &amp; regression residuals, net of fixed and random effects \
\hline
\end{tabular}
\end{table}</p>

<h1>Model Fitting</h1>

<p>The central concern in \texttt{WeMix} is properly incorporating sampling weights into the mixed model. Because each individual or group may have an unequal probability of selection into the sample, the estimate of the distribution of the MVN must include those weights to correctly estimate the parameters of the distribution. Also, except for trivial cases, the nested nature of the multilevel model creates a likelihood function that is not analytically calculable.</p>

<p>We consider the likelihood as a summation at each level, starting with the lowest level. The likelihood is conditional on the random effects at each higher level and is scaled by the weights at the lowest level.</p>

<p>In the case of the normal distribution (the only link function currently implemented in WeMix), the likelihood (\(\mathcal{L}^{(1)}\)) at the individual unit level is given by the equations below. Note that here the subscript \(i\) is used to indicate that this is the the likelihood of the \(i^{th}\) individual. 
\begin{equation}
\mathcal{L}<em>i<sup>{(1)}(\bm{\theta};\bm{y},\bm{U}<sup>{(l)},\bm{X},\bm{Z},\bm{W})</sup></sup> = \frac{1}{\Sigma<sup>{(1)}}</sup> \phi \left(  \frac{\hat{ e_i } }{\Sigma<sup>{(1)}}</sup>   \right) \
\end{equation}
Where \(\phi(\cdot)\) is the standard normal density function and \(\Sigma^{(1)}\) is the residual variance (a scalar). The residuals vector \(\hat{\bm{e}}\) represents the residuals \(\hat{e}_i\) for each individual, which is calculated as: 
\begin{equation}
\hat {\bm{e}} = \bm{y} - \bm{X}\hat{\bm{\beta}} - \sum</em>{l=2}<sup>L</sup> \bm{Z}<sup>{(l)}\hat{\bm{u}}<sup>{(l)}</sup></sup>  \ ,
\end{equation}</p>

<p>The conditional likelihood at each higher level is then recursively defined, for the $j$th unit, at level \(l\) (\(\mathcal{L}^{(l)}_j\); Rabe-Hesketh,Skrondal &amp; Pickles, 2002, eq. 3):<br/>
\begin{equation}
\mathcal{L}<sup>{(l)}_j(\bm{\theta};</sup> \bm{y},\bm{X}, \bm{Z},\bm{W}| \bm{U}<sup>{(l+1)})</sup> = \int<em>{-\infty}<sup>{\infty}</sup> &hellip; \int</em>{-\infty}<sup>{\infty}</sup> g<sup>{(l)}(\bm{u}<sup>{(l)})\prod_{j&#39;}{</sup></sup> \bigg[\mathcal{L}<sup>{(l-1)}_{j&#39;}(\bm{\theta}</sup> ; \bm{y},\bm{X}, \bm{Z},\bm{W}| \bm{U}<sup>{(l)})\bigg]<sup>{\bm{w}<sup>{(l-1)}_{j&#39;}}}</sup></sup></sup> du<em>1<sup>{(l)}</sup> &hellip; du</em>{k<em>l}<sup>{(l)}</sup>
\end{equation}
where the subscript \(j'\) that the product is indexed over indicates that the likelihood $\mathcal{L}<sup>{(l-1)}</sup></em>{j&#39;}(\cdot)$ is for the units of level \(l-1\) nested in unit \(j\). Additionally, \(g(\bm{u}^{(l)})\) is the empirical Bayes &ldquo;prior&rdquo; probability density of the random effects (at level \(l\)) having a value of \(u^{(l)}\), so \(g(\cdot)\) is a multivariate normal distribution parameterized by a mean of 0 and variance \(\Sigma\). The integrals are over the elements of \(\bm{u}=( u_1, &hellip;, u_{k_l})\), where \(k_l\) is the number of random effects at level \(l\). It is important to note that each \(\mathcal{L}^{(l)}\) is independent for each \(j'\). This allows us to integrate out the values of \(u\) for all groups simultaneously, which leaves us with a \(k_l\) dimensional integral and is essential to making this problem computationally feasible. At the highest level, the result is not conditional on any \(\bm{u}\) values, but is otherwise the same.</p>

<p>For ease of computation and in order to avoid problems with accurate storage of extremely small numbers we first take the log of the function before maximizing. The log-likelihood function is (Rabe-Hesketh 2006, eq. 1):
\begin{equation}
\ell<sup>{(l)}_j(\bm{\theta}</sup> ; \bm{y},\bm{X}, \bm{Z},\bm{W}| \bm{U}<sup>{(l+1)})</sup> = ln \left{ \int &hellip; \int {g<sup>{(l-1)}(u<sup>{(l-1)})</sup></sup> \cdot \exp \left[ \sum<em>{j&#39;}{\bm{w}<sup>{(l-1)}</sup></em>{j&#39;} \ell<sup>{(l-l)}_{j&#39;}(\bm{\theta}</sup> ; \bm{y},\bm{X}, \bm{Z},\bm{W}| U<sup>{(l)})</sup> } \right] du<em>1<sup>{(l)}</sup> &hellip; du</em>{k_l}<sup>{(l)}}</sup> \right}
\end{equation}</p>

<p>where \(\ell^{(l)}_j(\cdot)\) is the log-likelihood function for the $j$th unit at level \(l\).</p>

<p>Unfortunately, there is no closed-form expression for the integral in (10), and so we must evaluate the likelihood numerically. This is possible in with  adaptive Gauss-Hermite quadrature, which is a modification of Gauss-Hermite quadrature.</p>

<p>First, to evaluate the integral at level \(l\), we must evaluate the integral over \(k_l\) random effects variables that have a covariance matrix \(\bm{\Sigma}^{(l)}\). To avoid dependence, we use a change of variables to create independent and standard normal distributed vectors \(v^{(l)}\). Using the Cholesky decomposition \(\bm{\Sigma}^{(l)}=\bm{C}^{(l)} \left( \bm{C}^{(l)} \right)^T\), the value of \(u^{(l)}\) can then be calculated as \(u^{(l)} = \bm{C}^{(l)} \bm{v}^{(l)}\). </p>

<h2>Adaptive Gauss-Hermite Quadrature</h2>

<p>Using the transformation of variables described above, </p>

<p>\begin{align}
\ell<sup>{(l)}_j(\cdot)</sup> &amp; = \int&hellip;\int{g<sup>{(l)}}(u<sup>{(l)})</sup></sup> \cdot \exp \left[ \sum<em>{j&#39;}{{\bm{w}<sup>{(l-1)}</sup></em>{j&#39;} \ell<sup>{(l-l)}_{j&#39;}(\bm{\theta}</sup> ; \bm{y},\bm{X}, \bm{Z},\bm{W}| U<sup>{(l)})</sup> }} \right ] du<sup>{(l)}</sup> \
&amp;= \int\phi(v<em>{k_l}<sup>{(l)})&hellip;\int\phi(v</sup></em>{1}<sup>{(l)})</sup> \cdot \exp \left[\sum<em>{j&#39;}{{\bm{w}<sup>{(l-1)}</sup></em>{j&#39;} \ell<sup>{(l-l)}_{j&#39;}(\bm{\theta}</sup> ; \bm{y},\bm{X}, \bm{Z},\bm{W}| U<sup>{(l)})</sup> }} \right ] du<sup>{(l)}</sup> \label{eq:lnlv} \ ,
\end{align}
where \(u^{(l)}\) is now a function of the \(v\) values and \(\phi\) is the standard normal density.</p>

<p>Quadrature replaces integration with a summation over a finite set of points (known as quadrature points) and weights so that the sum approximates the integral&mdash;we annotate the quadrature points with a tilde so that, for example, \(u\) becomes \(\tilde u\). </p>

<p>These equations come from Rabe-Hesketh et al. (2002, p.5 eq. 4) and follow from eq. \ref{eq:lnlv}. </p>

<p>\begin{equation}
\ell<sup>{(l)}_j(\bm{\theta}</sup> ; \bm{y},\bm{X},\bm{Z},\bm{W}|\bm{U}<sup>{(l+1)}))</sup> =\ \sum<em>{r_1=1}<sup>Rp</sup></em>{r<em>1}<sup>{(l)}&hellip;\sum</sup></em>{r<em>{k_l}=1}<sup>{R}</sup> p</em>{r<em>{k_l}}<sup>{(l)}</sup> \cdot \exp \left[ \sum</em>{j&#39;}{\bm{w}<sup>{(l-1)}_{j&#39;}</sup> \ell<sup>{(l-l)}_{j&#39;}(\bm{\theta}</sup> ; \bm{y},\bm{X},\bm{Z},\bm{W}|\tilde{\bm{u}}<sup>{(l)},</sup> \bm{U}<sup>{(l+1)})}</sup> \right ] \ ,
\end{equation}
where \(R\) is the number of quadrature points, \(\bm{p}\) are the quadrature weights, \(\tilde{\bm{u}}^{(l)}= C^{(l)} \tilde{\bm{v}}^{(l)}\) are the quadrature point locations for each of the random effect vectors. This results in a grid with \(R\) quadrature points per element in \(u\) (and \(v\)), for a total of \(R^{k_l}\) quadrature points, and the summation is over every point in that grid. The quadrature points and weights come from the \texttt{statsmod} implementation of gaussian quadrature (Smyth 1998).</p>

<p>While Gauss-Hermite quadrature centers the quadrature points on the prior distribution, adaptive Gauss-Hermite quadrature (AGHQ) centers the quadrature points on either the likelihood surface or the posterior distribution. We use the posterior maximum (the likelihood function of which , including \(g(\cdot)\)), is detailed below, but this section is general to AGHQ as detailed in Lui and Pierce 1994, and Hartzel, Agresti, and Caffo, 2001; we use notation similar to Hartzel et al., 2001, p.87. The goal of adaptive quadrature is to reduce the number of quadrature points needed for an accurate evaluation of the integral by centering the points closer to the bulk of the integral. The location of the points is scaled based on the values of the likelihood function. To do this, the mode of the likelihood is used to center the points, and the dispersion is the estimated standard deviation of the likelihood at that point (using the inverse second derivative).</p>

<p>\begin{equation}
\tilde{\bm{u}} = \hat{\bm{\omega}}_j+\sqrt 2 \hat{\bm{Q}}<sup>{&frac12;}_j</sup> \tilde{\bm{v}}
\end{equation}</p>

<p>where \(\tilde{\bm{u}}_i^{(l)}\) are the quadrature points, \(\hat{\bm{\omega}}_i\) is a vector of locations for group \(j\), and \(\hat{\bm{Q}}_j\) is the matrix that is the inverse numerical second derivative of the likelihood function evaluated at the unit normal \(iid\) points \(\bm{v}\). </p>

<p>We can then use the adapted quadrature points to calculate the log likelihood as: </p>

<p>\begin{equation}
\begin{aligned}
\ell<sup>{(l)}_j</sup> (\bm{\theta} ; \bm{y},\bm{X}, \bm{Z},\bm{W}|
\bm{U}<sup>{(l+1)})</sup> ={} &amp; \
 \sum<em>{r_1=1}<sup>R</sup> p</em>{r<em>1}<sup>{(l)}</sup> &hellip; \sum</em>{r<em>{k_l}=1}<sup>{R}</sup> p</em>{r<em>{k_l}}<sup>{(l)}</sup>  \cdot \exp \left[  \sum</em>{j&#39;}{\bm{w}<sup>{(l-1)}_{j&#39;}</sup> \ell<sup>{(l-1)}_{j&#39;}(\bm{\theta}</sup> ; \bm{y},\bm{X} \bm{Z},\bm{W}|\tilde u<em>{r_1} &hellip; \tilde u</em>{r_{k_l}}, \bm{U}<sup>{(l+1)})}+g(\tilde{\bm{u}}<sup>{(l)};\bm{\Sigma}<sup>{(l)})+</sup></sup></sup> \bm{v}<sup>T</sup> \bm{v} \right]
\end{aligned}
\end{equation}</p>

<p>This approximation of the likelihood can then be evaluated and minimized numerically. In this package, we minimize the function using Newton&#39;s method. </p>

<h3>Calculation of Conditional Mode</h3>

<p>AGHQ requires an estimated location (\(\bm{\hat \omega}_j\)) and variance \(\bm{Q}_j\) for each group. These are calculated by iteratively finding the conditional mode of the random effects (to find \(\bm{\hat \omega}_j\)) and then using the inverse of the second derivative of the likelihood surface as an estimate of \(\bm{Q}_j\).</p>

<p>The conditional modes are identified by sequentially increasing levels of \(l\); the software first identifies the MAP at level 2, and then, using those estimates, uses AGHQ at level 2 to estimate conditional modes at level 3, and so on.</p>

<p>For each group, the conditional mode is identified using Newton&#39;s method on the likelihood for that unit (\(\ell^{(l)}_j(\cdot)\)) at a particular level of \(u^{(l)}\), called \(\hat{u}\), and conditional on an existing estimate of \(\theta\),
\begin{equation}
\ell<sup>{(l)}_j(\hat{\bm{u}}<sup>{(l)};</sup></sup> \bm{y},\bm{X}, \bm{Z},\bm{W}| \bm{\theta}) = ln \left[ {g<sup>{(l)}(\hat{\bm{u}}<sup>{(l)})\sum_{j&#39;}</sup></sup> {\bm{w}<sup>{(l-1)}_{j&#39;}</sup> \ell<sup>{(l-1)}_{j&#39;}</sup> (\bm{\theta} ; \bm{y}, \bm{X}, \bm{Z},\bm{W}| \hat{\bm{u}}<sup>{(l)})</sup> }} \right] \label{eq:map}
\end{equation}
where this formulation implicitly sets all values of \(\bm{U}^{(l)}\) to zero. Note that the values of \(\ell^{(l-l)}_{j'}\) still integrate out the values of \(\bm{u}\) for all levels below \(l\).</p>

<p>Newton&#39;s method requires a first and second derivative, and these are calculated with numerical derivatives of the likelihood calculated using numerical quadrature.</p>

<h3>Estimate of the Conditional Means</h3>

<p>The conditional mean is estimated by simply taking the expected value of each parameter using</p>

<p>\begin{equation}
E \left( \hat{\bm{u}}| \bm{y}, \bm{X}, \bm{Z}, \bm{W}, \bm{\theta} \right) = \frac{\int<em>{-\infty}<sup>{\infty}</sup> \tilde{\bm{u}}<sup>{(l-1)}</sup> \cdot \ell<sup>{(l)}_j(\hat{\bm{u}};</sup> \bm{y},\bm{X}, \bm{Z},\bm{W}| \bm{\theta}) d\hat{u}}{\int</em>{-\infty}<sup>{\infty}</sup> \ell<sup>{(l)}_j(\tilde{\bm{u}}<sup>{(l-1)};</sup></sup> \bm{y},\bm{X}, \bm{Z},\bm{W}| \bm{\theta}) d\hat{u}}
\end{equation}</p>

<h2>Variance Estimation of Random Effects: Sandwich Estimator</h2>

<p>The variance estimator was calculated following the method of Rabe-Hesketh &amp; Skrondal, 2006 . Thus, the variance is expressed as: </p>

<p>\begin{equation}
var(\bm{u}) = \bm{I<sup>{-1}J</sup> I<sup>{-1}}</sup>
\end{equation}</p>

<p>where \(\bm{I}\) is the pseudo Fisher information and calculated by observing the Fisher  information at the maximum likelihood estimates. Given that the likelihood is twice differentiable, we estimate the Fisher information as the second derivative (Hessian) of the likelihood evaluated at the maximum likelihood point. 
\begin{equation}
\bm{I} =  \frac{ \partial<sup>2</sup> \mathcal{L}(\cdot) }{\partial \theta<sup>2}</sup>
\end{equation}</p>

<p>and </p>

<p>\(\bm{J}\) is estimated as
\begin{equation}
\bm{J} = \sum<em>{L-1}{\frac{n</em>{L-1} }{n<em>{L-1} -1}} \sum</em>{j&#39;}{ \frac{ \partial \mathcal{L}<sup>{L}_{j}(\cdot)</sup> } {\partial \theta}
 \cdot  \left[ \frac{ \partial \mathcal{L}<sup>{L}_{j}(\cdot)</sup> } {\partial \theta}\right]<sup>{T</sup> }}
\end{equation}</p>

<p>where \(n_{L-1}\) represents the number of observations in the \((L-1)^{th}\) level (i.e., the second from the top level). And the subscript \(L-1\) indicates that the outermost sum is over the units \(j'\) in  \((L-1)^{th}\) level. </p>

<p>Here, the derivative of the likelihood function is calcuated numerically and evaluated at the maximum likelihood point estimates. </p>

<h1>Weight adjustments</h1>

<p>\texttt{WeMix} assumes that the weights are already scaled. However, for informational purposes, we describe here two common methods of scaling.</p>

<p>If there were no total nonresponses at any level, the ideal weights would simply be the inverse selection probabilities (Pfeffermann, Skinner, Holmes, Goldstein, &amp; Rasbash, 1998). But, because the samples are adjusted based on demographics, alternative weighting schemes are encouraged in the literature. Carle (2009) and Rabe-Hesketh et al. (2002) recommend that  sample weights be scaled; they cannot simply be the raw inverse probability of selection because this fails to adequately prevent bias when cluster sizes differ.  The notation is consistent with Carle, 2009. </p>

<p>Method A: 
\begin{equation}
w<sup>{*}_{ij}</sup> = w<em>{ij}\left( \frac{n_j}{\sum_i w</em>{ij}} \right) \ ,
\end{equation}
where \(w_{ij}\) are the full sample weights, \(i\) indexes the individuals, \(j\) indexes the groups, and \(n_j\) represents the number of observations in group \(j\).</p>

<p>Method B: 
\begin{equation}
w<sup>{*}_{ij}</sup> = w<em>{ij}\left(\frac{\sum_iw</em>{ij}}{\sum<em>iw</em>{ij}<sup>2}</sup> \right )
\end{equation}</p>

<p>These \(w^*\) are then used to scale the likelihood function. </p>

<h1>Appendix: Alternative Software Specifications</h1>

<p>For reference, these sections show the specification of the models in Stata&#39;s GLLAMM, Stata&#39;s MIXED, SAS PROC GLIMMIX, and HLM.</p>

<h2>Stata: GLLAMM</h2>

<p>In Stata prior to version 14, weighted mixed-effects models could be estimated only with GLLAMM (Rabe-Heskteh, Skrondal, &amp; Pickles, 2004). The work of GLLAMM authors Rabe-Hesketh, Skrondal, and Pickles provided the methods that we used in our implementation of weighted mixed models in \texttt{WeMix}. </p>

<pre><code class="r">import delimited &quot;PISA2012_USA.csv&quot;

generate intercept = 1
eq intercept: intercept
eq slope: escs
tabulate st29q03, generate (st29q03d)
tabulate sc14q02, generate (sc14q02d)
tabulate st04q01, generate (st04q01d)

//Random intercept model 
gllamm pv1math st29q03d1 st29q03d2 st29q03d4 sc14q02d1 sc14q02d3 sc14q02d4 st04q01d2 
escs, i(schoolid) pweight(pwt) l(identity) f(gau) nip(27) nrf(1) eqs(intercept) 
adapt nocorrel


//Random slope and intercept model 
gllamm pv1math st29q03d1 st29q03d2 st29q03d4 sc14q02d1 sc14q02d3 sc14q02d4 st04q01d2
escs, i(schoolid) pweight(pwt) l(identity) f(gau) nip(13) nrf(2) eqs(intercept slope) 
adapt nocorrel
</code></pre>

<h2>Stata: MIXED</h2>

<p>In Stata Version 14, the MIXED command includes the ability to fit models with survey weights (StataCorp, 2015). </p>

<pre><code class="r">import delimited &quot;PISA2012_USA.csv&quot;
tabulate st29q03, generate (st29q03d)
tabulate sc14q02, generate (sc14q02d)
tabulate st04q01, generate (st04q01d)

//Random intercept model 
mixed pv1math st29q03d1 st29q03d2 st29q03d4 sc14q02d1 sc14q02d3 sc14q02d4 st04q01d2 
escs [pw = pwt1] || schoolid: , pweight (pwt2)

//Random slope and intercept model 
mixed pv1math st29q03d1 st29q03d2 st29q03d4 sc14q02d1 sc14q02d3 sc14q02d4 st04
q01d2 escs [pw = pwt1] || schoolid: escs, pweight (pwt2)
</code></pre>

<h2>SAS</h2>

<p>Model specification in SAS  uses the GLIMMIX procedure. It is notable here that when fit with the default optimization parameters, the model converged to a likelihood lower than the maximum likelihood estimate found by other software. Decreasing the convergence parameter GCONV to E-10 was necessary to find the same maximum likelihood as other software. </p>

<pre><code class="r">proc import datafile=&quot;PISA2012_USA.csv&quot;
     out=pisa_data
     dbms=csv
     replace;
run;

proc glimmix data=pisa_data method=quadrature(qpoints=27) empirical=classical NOREML; 
  nloptions GCONV=1E-10 technique=TRUREG;
    class  sc14q02(ref=&#39;Not at all&#39;) st04q01(ref=&#39;Female&#39;) st29q03(ref=&#39;Strongly agree&#39;);
    model pv1math = escs sc14q02 st04q01 st29q03/  obsweight=pwt1 solution;
    random INT/subject=schoolid weight=pwt2;
run;


proc glimmix data=pisa_data method=quadrature(qpoints=13) empirical=classical NOREML;
  nloptions GCONV=1E-10 technique=TRUREG;
  class  sc14q02(ref=&#39;Not at all&#39;) st04q01(ref=&#39;Female&#39;) st29q03(ref=&#39;Strongly agree&#39;);
  model pv1math = escs sc14q02 st04q01 st29q03/ obsweight=pwt1 solution;
  random intercept escs/subject=schoolid   weight=pwt2;
run;
</code></pre>

<h2>HLM</h2>

<p>HLM is another software package for estimated mixed-effects models (Raudenbush, Bryk, Cheong,  Congdon, &amp; Toit, 2016). It is important to note that HLM has two differences from the methods specified in other softwares. HLM normalizes all weights (which other programs do not) and also does not allow the correlation between slope and random effect to be fixed at 0. Using the &ldquo;Diagonalize Tau&rdquo; option reduces covariance, but does not fix it at 0 (Raudenbush et al., 2016). In addition, HLM is entirely graphical user interface (GUI) based. Specification of the HLM model for comparison here was done through the interface. The random intercept model was specified as: </p>

<p>\includegraphics[height=7cm]{hlm1}</p>

<p>And the random slope and intercept model was specified as: </p>

<p>\includegraphics[height=7cm]{hlm2}</p>

<p>Note: The specifications for the random intercept model and the random slope model are extremely similar; in the second image for  \(\beta_1\), the \(u_1\) is highlighted. This is how users in HLM add random effects. </p>

<h1>Appendix: Example Data Preparation</h1>

<p>Data are read in using the \texttt{EdSurvey} package to access the PISA data efficiently. </p>

<pre><code class="r">library(edsurvey)

#read in data 
cntl = readPISA([path], countries = &quot;USA&quot;)
om &lt;- getAttributes(cntl, &quot;omittedLevels&quot;)
data &lt;- getData(cntl, c(&quot;schoolid&quot;,&quot;pv1math&quot;,&quot;st29q03&quot;,&quot;sc14q02&quot;,&quot;st04q01&quot;,&quot;escs&quot;,&quot;w_fschwt&quot;,&quot;w_fstuwt&quot;), omittedLevels = FALSE, addAttributes = FALSE)

#prepare weights 
data$sqw &lt;- data$w_fstuwt^2
sumsqw &lt;- aggregate(sqw ~ schoolid, data = data, sum)
sumw &lt;- aggregate(w_fstuwt ~ schoolid, data = data, sum)
data$sumsqw &lt;- sapply(data$schoolid, function(s) sumsqw$sqw[sumsqw$schoolid == s])
data$sumw &lt;- sapply(data$schoolid, function(s) sumw$w_fstuwt[sumw$schoolid == s])
data$pwt1 &lt;- data$w_fstuwt * (data$sumw / data$sumsqw)
data$pwt2 &lt;- data$w_fschwt

# Remove NA and omitted Levels
om &lt;- c(&quot;Invalid&quot;,&quot;N/A&quot;,&quot;Missing&quot;,&quot;Miss&quot;,NA,&quot;(Missing)&quot;)
for (i in 1:ncol(tempData)) {
  tempData &lt;- tempData[!tempData[,i] %in% om,]
}

#relevel factors for model 
data$st29q03 &lt;- relevel(data$st29q03,ref=&quot;Strongly agree&quot;)
data$sc14q02 &lt;- relevel(data$sc14q02,ref=&quot;Not at all&quot;)
</code></pre>

<h1>Citations</h1>

<p>Bates, D., Maechler, M., Bolker, B., &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1-48.</p>

<p>Carle, A. C. (2009). Fitting multilevel models in complex survey data with design weights: Recommendations. BMC Medical Research Methodology, 9(1).</p>

<p>Hartzel, J., Agresti, A., &amp; Caffo, B. (2001). Multinomial logit random effects models. Statistical Modelling:
An International Journal, 1(2), 81-102.</p>

<p>Liu, Q., &amp; Pierce, D. A. (1994). A note on Gauss-Hermite quadrature. Biometrika, 81(3), 624.</p>

<p>OECD. (2013), PISA 2012 assessment and analytical framework: Mathematics, reading, science, problem solving and financial literacy. Paris, France. OECD Publishing. Retrieved from <a href="http://www.oecd.org/pisa/pisaproducts/PISA%202012%20framework%20e-book_final.pdf">http://www.oecd.org/pisa/pisaproducts/PISA%202012%20framework%20e-book_final.pdf</a></p>

<p>Pfeffermann, D., Skinner, C., Holmes, D., Goldstein, H., &amp; Rasbash, J. (1998). Weighting for unequal
selection probabilities in multilevel models. Journal of the Royal Statistical Society. Series B (Statistical
Methodology), 60(1), 23-40.</p>

<p>Rabe-Hesketh, S., &amp; Skrondal, A. (2006). Multilevel modelling of complex survey data. Journal of the Royal Statistical Society. Series A (Statistics in Society), 169(4), 805-827.</p>

<p>Rabe-Heskteh, S., Skrondal, A., &amp; Pickles, A. (2004). GLLAMM manual (Working Paper No. 160). Berkeley, CA: University of California, Berkeley, Division of Biostatistics.</p>

<p>Rabe-Hesketh, S., Skrondal, A., &amp; Pickles, A. (2005). Maximum likelihood estimation of limited and discrete dependent variable models with nested random effects. Journal of Econometrics, 128(2), 301-323.</p>

<p>Rabe-Hesketh, S., Skrondal, A., &amp; Pickles, A. (2002). Reliable estimation of generalized linear mixed models using adaptive quadrature. Stata Journal, 2, 1-21.</p>

<p>Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (2nd ed.). Thousand Oaks, CA: SAGE Publications.</p>

<p>Raudenbush, S. W., Bryk, A., Cheong, Y. F., &amp; Congdon, R. (2005). HLM 6: Hierarchical linear and nonlinear modeling. Lincolnwood, IL: Scientific Software International. </p>

<p>Raudenbush, S. W., Bryk, A. S., Cheong, Y. F., Congdon, R. T., &amp; Toit, M. (2016). HLM7 hierarchical linear and nonlinear modeling user manual: User guide for Scientific Software International&#39;s (S.S.I.) Program. Lincolnwood, IL: Scientific Software International.</p>

<p>SAS Institute, Inc. (2013). SAS/ACCESS 9.4 interface to ADABAS: Reference. Cary, NC: Author.</p>

<p>Scientific Software International. (2016). Design weighting in the hierarchical context. Retrieved from <a href="http://www.ssicentral.com/hlm/example6-2.html">www.ssicentral.com/hlm/example6-2.html</a> </p>

<p>Smyth, G. K. (1998). Numerical integration. In P. Armitage &amp; T. Colton (Eds.), Encyclopedia of Biostatistics (pp. 3088-3095). London, UK: Wiley. </p>

<p>StataCorp. (2015). Stata Statistical Software: Release 14. College Station, TX: Author.</p>

</body>

</html>
